---
title: "heidi_fits"
author: "Victor Navarro"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{heidi_fits}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  fig.width = 7,
  collapse = TRUE,
  comment = "#>",
  message = F,
  warning = F
)
```

```{r setup}
library(heidi)
library(ggplot2)
library(magrittr)
library(dplyr)
library(tidyr)
theme_set(theme_bw())
data(pati)
set.seed(2022)
```

# Fitting the model to empirical data

We now fit the model to some empirical data (Patitucci et al., 2016, Experiment 1). This will involve writing a function that produces model responses organized as the empirical data, and using that function for maximum likelihood estimation (MLE). We begin with a short overview of the data, then move to the model function, and finally fit the model.

## The data

The data (`pati`) contains the responses (lever presses or lp, and nose pokes or np) for 32 subjects (rats) across 6 blocks of training (2 sessions per block). The animals were trained to associate each of two levers to one of two unconditioned stimuli (pellets or sucrose). Let's take a look at it.

```{r}
glimpse(pati)
pati  %>% ggplot(aes(x = block, y = rpert, colour = us)) +
  geom_line(aes(group = interaction(us, subject)), alpha = .3) +
  stat_summary(geom = 'line', fun = 'mean', size = 1) +
  labs(x = "Block", y = "Responses per trial", colour = "US") +
  facet_grid(~response)
```

The thicker lines are group averages; the rest are individual subjects. We ignore the specific mapping between levers and USs here, because that was counterbalanced across subjects. However, the counterbalancing will end up being relevant (see ahead).

## Writing the model function

The biggest hurdle in fitting the model to empirical data is to write a function that, given a vector of parameters and model arguments, generates responses that are organized as the empirical data. Let's begin by summarizing the group data first, so we know what to aim for.

```{r}
pati_summ <- pati %>% 
  group_by(block, us, response) %>%
  summarise(rpert = mean(rpert), .groups = "drop")
head(pati_summ)
```

We now prepare the arguments for the model function (as you would pass to `run_heidi`). To achieve this, we will use `make_heidi_args`, a function that requires three other bits: 1) a design data.frame, 2) a table with parameters, and 3) a list with simulation options. Note that these arguments are fixed during the optimization process, which means that the training routine within each iteration of the numerical optimization is always the same.

This is no minor issue, because the HeiDI is sensitive to order effects. Hence, it is important that the arguments we prepare here reflect the behavior of the model after an "general" experimental procedure, and not the quirks of an unfortunate run of tials. Here, we simply address this issue by running several iterations of the model (with random trial orders) and average all models before evaluating the likelihood of the parameters.

So what do we have to design? The experiment presented in Patittuci et al. (2016) was fairly simple, and it can be reduced to the presentations of two levers, each followed by a different appetitive outcome. Here, we will assume that the two outcomes are independent from each other. We will also take some liberties with the number of trials we specify in order to reduce computing time.


```{r}
#The design data.frame
des_df <- data.frame(group = c("CB1", "CB2"),
                     training = c("12L>(Pellet)/12R>(Sucrose)", "12L>(Sucrose)/12R>(Pellet)"),
                     rand_train = T)
#The parameters
pars <- get_params(des_df) #the actual parameter values don't matter, as our function will re-write them inside the optimizer call
#The options
opts <- get_heidi_opts(iterations = 10)
#The arguments
my_mod_args <- make_heidi_args(design = parse_design(des_df), pars = pars, opts = opts)

```

Note we specified two counterbalancings as groups. It is very important that we reproduce the counterbalancings in the data we are trying to fit as close as possible. Otherwise, the optimization process might latch onto experimentally-irrelevant variables. For example, it can be seen in `pati` that there was more lever pressing whenever a lever was paired with pellets. If we didn't counterbalance the identities of the levers and USs, the optimization might result into one of the levers being less salient than the other.

We can now begin to write the model function. First, it would be a good idea to see what `run_heidi` returns if run with the arguments above.

```{r}
mod_res <- run_heidi(args = my_mod_args)
str(mod_res)
```

Although the `run_heidi` function returns a list with 4 tibbles, we only care about one of them: `rs` (the model responses). With that in hand, we can write our model function.

```{r}
my_model_function <- function(pars, model_args){
  #manipulating pars
  names(pars) = names(model_args$stim_alphas[[1]])
  model_args$stim_alphas = list(pars)
  #running the model and selecting rs
  mod_res = run_heidi(args = model_args)$rs
  #summarizing the model
  mod_res = mod_res %>%
    filter(s2 %in% c("Pellet", "Sucrose")) %>%
    mutate(response = ifelse(s1 %in% c("Pellet", "Sucrose"), "np", "lp"),
           block = ceiling(trial/4)) %>%
    rowwise() %>%
    #note this filter below; we do not allow lever presses if the lever was not presented on the trial
    filter(response == "np" | (response == "lp" & grepl(s1, trial_type))) %>%
    mutate(us = ifelse(grepl("Pellet", trial_type), "P", "S")) %>%
    group_by(us, block, response) %>%
    summarise(value = mean(value), .groups = "drop")
  mod_res
}

```

Let's dissect the function above in its three parts. 

1. We do some manipulation on the vector of parameters, naming them according to the stimulus names identified in the arguments (`names(model_args$stim_alphas[[1]])`). We do this because the function we use to simulate (i.e., the one called by `run_heidi`) requires a named vector to work, and some optimizers (looking at you optim) strip the names from the numerical vectors they are trying to optimize. 

2. We run the model and immediately select the relevant information (rs).

3. Finally, we summarise the model responses, taking care of the different counterbalancings in the process. Within this step, we also filter all output nodes that are not related to expecting one of the USs (because the latest public version of the model is lagging behind the latest theoretical developments), we classify responses as being nosepokes (produced by the US) or lever presses (produced by the levers), and calculate the mean across blocks of trials.

Let's see the function in action.

```{r}
my_model_function(c(.1, .2, .4, .3), model_args = my_mod_args)
```

And just as a refresher, here's the summarised empirical data.

```{r}
pati_summ
```

Do you notice anything odd about the ordering? The empirical data is sorted in a different way, but I said before that the order of the empirical data and model responses must match. I cannot emphasize this point enough: there is nothing within the fit function that checks or reorders the data for you. You are the sole responsible for making sure both of these pieces of data are in the same order.

Here, we will simply rewrite the model function so it matches the ordering of the empirical data. Notice the different order in the `group_by` line below.


```{r}
my_model_function <- function(pars, model_args){
  #manipulating pars
  names(pars) = names(model_args$stim_alphas[[1]])
  model_args$stim_alphas = list(pars)
  #running the model and selecting rs
  mod_res = run_heidi(args = model_args)$rs
  #summarizing the model
  mod_res = mod_res %>%
    filter(s2 %in% c("Pellet", "Sucrose")) %>%
    mutate(response = ifelse(s1 %in% c("Pellet", "Sucrose"), "np", "lp"),
           block = ceiling(trial/4)) %>%
    rowwise() %>%
    #note this filter below; we do not allow lever presses if the lever was not presented on the trial
    filter(response == "np" | (response == "lp" & grepl(s1, trial_type))) %>%
    mutate(us = ifelse(grepl("Pellet", trial_type), "P", "S")) %>%
    group_by(block, us, response) %>%
    summarise(value = mean(value), .groups = "drop")
  mod_res
}

glimpse(my_model_function(c(.1, .2, .4, .3), model_args = my_mod_args))
glimpse(pati_summ)
```

Much better! We are now ready to begin fitting the model.

## Fitting the model

We fit models using the `fit_heidi` function. This function requires 4 arguments: 

1. The (empirical) data
2. A model function
3. The arguments with which to run the model function.
4. The optimizer options.

We have done a great job taking care of the first three, so let's tackle the last.

```{r}
my_optimizer_opts <- get_optimizer_opts(optimizer = "optim",
                                        stim_names = pars$Stimulus,
                                        family = "linear")
my_optimizer_opts
```

The `get_optimizer_opts` function returns many things:

1. stim_names: The name of the stimuli for which to find salience parameters.
2. lower and upper: The lower and upper bounds for the parameter search. Consider shrinking these to speed up the process.
3. optimizer: The numerical optimization technique we wish to use during MLE estimation.
4. sample_pars: A function that samples parameters from set distributions.
5. family: The family distribution we assume for our model. In practice, what you request here will be used to determine the link function to transform model responses, and the likelihood function used in the objective function. The linear family here does nothing fancy to the model responses, and will estimate an extra parameter, scale, which scales the model responses into (roughly) the scale of the empirical data. When it comes to likelihood functions, this family will use the normal density of the data and model differences.
6. family_pars: The family-specific parameter being estimated alongside salience parameters.
7. verbose: Whether to print parameters and objective function values as we optimize.
8. optim_options: The optimizer-specific options that are used in the optimization call.

You are free to modify these; just make sure the structure of the list returned by `get_optimizer_opts` remains the same. Here, I overwrite the trace argument passed to the optimizer.

```{r}
my_optimizer_opts$optim_options$control$trace = 0
```

And with that, we can fit the model!

```{r, eval=F, include=T}
the_fit <- fit_heidi(pati_summ$rpert,
                     model_function = my_model_function, model_args = my_mod_args,
                     optimizer_options = my_optimizer_opts)
```

```{r, include = F, echo = F}
#save("the_fit", file = "heidi_fits_fit.rda")
load(file = "heidi_fits_fit.rda")
```


The `fit_heidi` function returns a lot of information to track what we put in and what we got out. Regarding the latter, we can see the MLE parameters we obtained this time, and their negative log likelihood, given the data:

```{r}
the_fit[c("best_pars", "nloglik")]
```

That's good and all, but how well does a model run with those parameters "visually" fit the data? We can obtain the predictions from the model via the `fit_predict` function.

```{r}
prediction = fit_predict(the_fit) %>%
  group_by(block, us, response) %>%
  summarise(value = mean(value), .groups = "drop")
prediction$data = the_fit$data
prediction %>% rename("prediction" = "value") %>%
    pivot_longer(cols = c("prediction", "data"),
                        names_to = "type",
                        values_to = "value") %>%
    ggplot(ggplot2::aes(x = block, y = value, colour = us, linetype = type)) +
    geom_line() +
    theme_bw() + 
    facet_grid(us~response)
```

This looks pretty good! Save from some blatant misfits, of course. Now you know everything you need to fit heidi to your empirical data. Go forth!

### A final note
This vignette was pre-generated, as I don't want the user to fit the model at the time of installation. I will try to keep up with it as the package develops, but if you spot any inconsistencies, please drop me a line.
